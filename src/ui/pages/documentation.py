"""
Documentation page for the application.
"""
import streamlit as st


def render_documentation():
    """Render the documentation tab."""
    st.header("ğŸ“– Documentation")
    
    # Overview Section
    st.markdown("""
    ## ğŸ—ï¸ Architecture Overview
    
    This application demonstrates **five scenarios** for using Bing Grounding with Azure AI Foundry Agents.
    Each scenario explores different architectural patterns for integrating real-time web search into AI agents.
    """)
    
    # Scenario 1
    st.markdown("---")
    st.subheader("ğŸ“Œ Scenario 1: Direct Agent with Bing Tool")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        **Architecture:** `User â†’ Agent (Bing Tool Attached) â†’ Bing API â†’ Results`
        
        The simplest pattern where the Bing Grounding tool is directly attached to the agent at creation time.
        
        **How it works:**
        1. User submits a company analysis request
        2. App creates/reuses an agent with Bing grounding tool attached
        3. Agent searches using the native Bing grounding capability
        4. Citations returned as URL annotations in response
        
        **Key Characteristics:**
        - âœ… Lowest latency
        - âœ… Simplest implementation
        - âš ï¸ Market configured at tool creation time (not runtime)
        """)
    
    with col2:
        st.code("""
User
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Direct Agentâ”‚
â”‚ (Bing Tool) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bing API   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """, language="text")
    
    # Scenario 2
    st.markdown("---")
    st.subheader("ğŸ“Œ Scenario 2: Two-Agent Pattern via MCP Server")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        **Architecture:** `User â†’ Orchestrator Agent â†’ MCP Tool â†’ Worker Agent (Bing) â†’ Results`
        
        An orchestrator agent delegates search to ephemeral worker agents created via MCP.
        
        **How it works:**
        1. Orchestrator agent receives analysis request
        2. Orchestrator calls MCP tool `create_and_run_bing_agent`
        3. MCP server creates a Worker Agent with market-specific Bing tool
        4. Worker executes search and returns results
        5. MCP server deletes the worker agent (ephemeral)
        6. Results flow back through orchestrator
        
        **Key Characteristics:**
        - âœ… Dynamic market configuration at runtime
        - âœ… Isolated worker agents per request
        - âš ï¸ Higher latency (agent creation overhead)
        """)
    
    with col2:
        st.code("""
User
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestratorâ”‚
â”‚   Agent     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ MCP Call
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Server  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Creates
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker Agentâ”‚
â”‚ (Bing Tool) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bing API   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """, language="text")
    
    # Scenario 3
    st.markdown("---")
    st.subheader("ğŸ“Œ Scenario 3: Agent â†’ MCP Tool â†’ REST API")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        **Architecture:** `User â†’ Agent (MCP Tool) â†’ MCP Server â†’ Bing REST API â†’ Results`
        
        Agent uses MCP tool that directly calls the Bing Grounding REST API without creating nested agents.
        
        **How it works:**
        1. Agent with MCP tool receives request
        2. Agent calls `bing_search_rest_api` MCP tool with market parameter
        3. MCP server makes direct POST to `/openai/responses` with `bing_grounding` tool
        4. REST API returns grounded results with citations
        5. MCP server formats and returns results
        
        **Key Characteristics:**
        - âœ… Direct REST API access (no nested agents)
        - âœ… Full control: count, freshness, setLang parameters
        - âœ… Citations extracted from REST response
        """)
    
    with col2:
        st.code("""
User
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Agent   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ MCP Call
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Server  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ REST API
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bing REST   â”‚
â”‚    API      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """, language="text")
    
    # Scenario 4
    st.markdown("---")
    st.subheader("ğŸ“Œ Scenario 4: Multi-Market Sequential Search")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        **Architecture:** `User â†’ Agent â†’ MCP Tool (called N times) â†’ Aggregated Results`
        
        Single agent makes multiple sequential tool calls for different markets.
        
        **How it works:**
        1. User selects multiple markets (e.g., en-US, de-DE, ja-JP)
        2. Agent receives prompt instructing N separate tool calls
        3. Agent calls MCP tool sequentially for each market
        4. Agent aggregates results and provides cross-market analysis
        
        **Key Characteristics:**
        - âœ… Simple single-agent approach
        - âš ï¸ Sequential execution (slower for many markets)
        - âš ï¸ All-or-nothing failure mode
        - ğŸ“Š Best for 2-3 markets
        """)
    
    with col2:
        st.code("""
User
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Marketâ”‚
â”‚    Agent    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â”‚ Loop  â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜
       â”‚ Sequential
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Server  â”‚â—„â”€â”
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
       â”‚         â”‚
       â–¼         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ Bing REST   â”‚â”€â”€â”˜
â”‚ (market N)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """, language="text")
    
    # Scenario 5
    st.markdown("---")
    st.subheader("ğŸ“Œ Scenario 5: Workflow-Based Parallel Multi-Market")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        **Architecture:** `User â†’ Dispatcher â†’ Parallel Searches â†’ Aggregator â†’ Analysis Agent â†’ Results`
        
        Structured workflow with parallel execution and dedicated analysis phase.
        
        **Workflow Stages:**
        1. **Stage 1 - Dispatch:** Split request into parallel market tasks
        2. **Stage 2 - Parallel Search:** Execute all markets concurrently (90s timeout each)
        3. **Stage 3 - Aggregation:** Collect results, handle failures gracefully
        4. **Stage 4 - Analysis:** Dedicated agent synthesizes cross-market findings
        
        **Key Characteristics:**
        - âœ… **3-5x faster** than sequential (parallel execution)
        - âœ… Per-market timeout handling (90s default)
        - âœ… Graceful degradation on failures
        - âœ… Dedicated analysis agent (no tools, pure synthesis)
        - ğŸ“Š Best for production multi-market research
        """)
    
    with col2:
        st.code("""
User
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dispatcher â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
  â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”  Parallel
  â–¼    â–¼    â–¼
â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â”
â”‚US â”‚â”‚DE â”‚â”‚JP â”‚
â””â”€â”¬â”€â”˜â””â”€â”¬â”€â”˜â””â”€â”¬â”€â”˜
  â”‚    â”‚    â”‚
  â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aggregator  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analysis   â”‚
â”‚    Agent    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """, language="text")
    
    # Comparison Table
    st.markdown("---")
    st.subheader("ğŸ“Š Scenario Comparison")
    
    st.markdown("""
    | Feature | Scenario 1 | Scenario 2 | Scenario 3 | Scenario 4 | Scenario 5 |
    |---------|:----------:|:----------:|:----------:|:----------:|:----------:|
    | **Pattern** | Direct | Two-Agent | MCP REST | Multi-Market | Workflow |
    | **Markets** | Single | Single | Single | Multiple | Multiple |
    | **Execution** | Sync | Sync | Sync | Sequential | **Parallel** |
    | **Timeout Handling** | Basic | Basic | Basic | Limited | **Per-market** |
    | **Failure Mode** | All-or-nothing | All-or-nothing | All-or-nothing | All-or-nothing | **Graceful** |
    | **Latency** | âš¡ Lowest | Medium | Medium | High | **Fast** |
    | **Complexity** | Low | Medium | Medium | Medium | High |
    """)
    
    # Module Structure
    st.markdown("---")
    st.subheader("ğŸ“ Module Structure")
    
    st.code("""
src/
â”œâ”€â”€ core/                  # Domain models & interfaces
â”‚   â”œâ”€â”€ models.py          # Pydantic models (Citation, AnalysisResponse, etc.)
â”‚   â””â”€â”€ interfaces.py      # Abstract base classes
â”œâ”€â”€ infrastructure/        # Azure clients, config, tracing
â”‚   â”œâ”€â”€ azure_client.py    # AIProjectClient factory
â”‚   â”œâ”€â”€ config.py          # Environment configuration
â”‚   â””â”€â”€ tracing.py         # OpenTelemetry setup
â”œâ”€â”€ services/              # Business logic
â”‚   â”œâ”€â”€ agent_service.py   # Agent lifecycle management
â”‚   â””â”€â”€ risk_analyzer.py   # Prompt generation
â”œâ”€â”€ scenarios/             # Scenario implementations
â”‚   â”œâ”€â”€ base.py            # BaseScenario abstract class
â”‚   â”œâ”€â”€ scenario1_direct.py
â”‚   â”œâ”€â”€ scenario2_mcp_agent.py
â”‚   â”œâ”€â”€ scenario3_mcp_rest.py
â”‚   â”œâ”€â”€ scenario4_multi_market.py
â”‚   â””â”€â”€ scenario5_workflow.py
â””â”€â”€ ui/                    # Streamlit UI
    â”œâ”€â”€ app.py             # Main entry (<100 lines)
    â”œâ”€â”€ components/        # Sidebar, shared components
    â””â”€â”€ pages/             # Scenario-specific pages
    """, language="text")
    
    # SOLID Principles
    st.markdown("---")
    st.subheader("ğŸ›ï¸ SOLID Principles Applied")
    
    st.markdown("""
    - **Single Responsibility**: Each scenario file handles one integration pattern
    - **Open/Closed**: New scenarios extend `BaseScenario` without modifying existing code
    - **Liskov Substitution**: All scenarios implement the same `execute()` interface
    - **Interface Segregation**: Separate interfaces for client factory, risk analysis
    - **Dependency Inversion**: Scenarios depend on `IAzureClientFactory` abstraction
    """)
    
    # Citation Handling
    st.markdown("---")
    st.subheader("ğŸ”— Citation Handling")
    
    st.markdown("""
    Citations are extracted from two sources depending on the scenario:
    
    **1. URL Annotations (Scenario 1 - Direct Bing Grounding)**
    ```python
    # Citations in response.output[].content[].annotations[]
    for annotation in content.annotations:
        if hasattr(annotation, 'url'):
            citations.append(Citation(url=annotation.url, title=annotation.title))
    ```
    
    **2. MCP Tool JSON Response (Scenarios 2-5)**
    ```python
    # Citations embedded in JSON response from MCP tool
    data = json.loads(content.text)
    for cit in data.get('citations', []):
        citations.append(Citation(url=cit['url'], title=cit['title']))
    ```
    """)
    
    # Running the Application
    st.markdown("---")
    st.subheader("ğŸš€ Running the Application")
    
    st.code("""
# Install dependencies
pip install -r requirements.txt

# Set environment variables
export AZURE_AI_PROJECT_ENDPOINT="https://your-project.services.ai.azure.com"
export AZURE_AI_MODEL_DEPLOYMENT_NAME="gpt-4o"
export BING_PROJECT_CONNECTION_NAME="your-bing-connection"
export MCP_SERVER_URL="https://your-mcp-server.azurewebsites.net/mcp"

# Run the app
streamlit run src/ui/app.py
    """, language="bash")
